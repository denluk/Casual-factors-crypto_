{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "os.chdir('/Users/denislukanov/Desktop/Causal/data')\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-deep')\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "import datetime\n",
    "import math\n",
    "import sklearn\n",
    "import pandas_datareader.data as web\n",
    "import sklearn.discriminant_analysis\n",
    "#from pandas.io.data import DataReader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.svm import LinearSVC, SVC\n",
    "# from pandas_datareader import data as pdr\n",
    "# import yfinance as yfin\n",
    "import fattails.metrics as fattails\n",
    "import lightgbm as lgb\n",
    "#yfin.pdr_override()\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c727fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('BTCUSDT_15M.csv', index_col=0)\n",
    "data1=data1[['open', 'high', 'low', 'close', 'vol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda90ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=data1\n",
    "\n",
    "ts['pct_change'] = ts.close.pct_change()\n",
    "ts['survival_probability'] = fattails.get_survival_probability(ts['pct_change'])\n",
    "ts['dollar_value']=ts['close']*ts['vol']\n",
    "ts['dollar_value_ch']=ts['dollar_value']-ts['dollar_value'].shift(1)\n",
    "ts['log_ret']=np.log(ts['close']).diff()\n",
    "ts=ts.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap_OC(df_open,df_close):    \n",
    "    dif = []\n",
    "    for i in range(len(df_open)):\n",
    "        a=df_open[i]-df_close[i-1]\n",
    "        dif.append(a)\n",
    "    return dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_method(high, low, close, window_size):\n",
    "    \n",
    "    # Calculate the rolling average of high and low prices for the last window_size days\n",
    "    high_rolling_mean = high.rolling(window=window_size).mean()\n",
    "    low_rolling_mean = low.rolling(window=window_size).mean()\n",
    "    \n",
    "    # Convert the close column to float64 data type\n",
    "    close = close.astype(float)\n",
    "    \n",
    "    # Calculate the R method value\n",
    "    R_method = (high_rolling_mean - close) / (high_rolling_mean - low_rolling_mean)\n",
    "    \n",
    "    # Return the R method value\n",
    "    return R_method\n",
    "R = R_method(high=data1['high'], low=data1['low'], close=data1['close'], window_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92655e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historicalVaR(returns, alpha=5):\n",
    "    \"\"\"\n",
    "    Read in a pandas dataframe of returns / a pandas series of returns.\n",
    "    Output the percentile of the distribution at the given alpha confidence level.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        return np.percentile(returns, alpha)\n",
    "    \n",
    "    # A passed user-defined-function will be passed a Series for evaluation.\n",
    "    \n",
    "    elif isinstance(returns, pd.DataFrame):\n",
    "        return returns.aggregate(historicalVaR, alpha=alpha)\n",
    "   \n",
    "    else:\n",
    "        raise TypeError(\"Expected returns to be dataframe or series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def historicalCVaR(returns, alpha=5):\n",
    "    \"\"\"\n",
    "    Read in a pandas dataframe of returns / a pandas series of returns\n",
    "    Output the CVaR for dataframe / series\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        belowVaR = returns <= historicalVaR(returns, alpha=alpha)\n",
    "        return returns[belowVaR].mean()\n",
    "    \n",
    "    # A passed user-defined-function will be passed a Series for evaluation.\n",
    "    \n",
    "    elif isinstance(returns, pd.DataFrame):\n",
    "        return returns.aggregate(historicalCVaR, alpha=alpha)\n",
    "   \n",
    "    else:\n",
    "        raise TypeError(\"Expected returns to be dataframe or series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487af20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDailyVol(close,span0=100):\n",
    "    close.index = pd.to_datetime(close.index)\n",
    "    \n",
    "    df0=close.index.searchsorted(close.index-pd.Timedelta(days=3))\n",
    "    df0=df0[df0>0]\n",
    "    df0=pd.Series(close.index[df0-1],index=close.index[close.shape[0]-df0.shape[0]:])\n",
    "    df0=close.loc[df0.index]/close.loc[df0.values].values-1#daily returns\n",
    "    df0=df0.ewm(span=span0).std()\n",
    "    #df0=df0.rename(columns={'Close':'volatilities'})\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_OC=get_gap_OC(ts['open'],ts['close'])\n",
    "ts['gap_OC']=gap_OC\n",
    "ts['gap_OC'][0]=0\n",
    "ts['H-L_spread_change']=(ts['high']-ts['low'])/ts['close']\n",
    "ts['O-C_spread_change']=(ts['open']-ts['close'])/ts['close']\n",
    "ts['R_indicator'] = R\n",
    "\n",
    "ts['rolling_dollar_volume']=ts['dollar_value'].rolling(16).mean() \n",
    "ts['CVar_16']=ts['pct_change'].rolling(16).apply(historicalCVaR)\n",
    "ts['CVar_4']=ts['pct_change'].rolling(4).apply(historicalCVaR)\n",
    "ts[\"Direction\"] = np.sign(ts[\"pct_change\"])\n",
    "ts[\"Direction_1Lag\"] = np.sign(ts[\"pct_change\"].shift(+1))\n",
    "ts[\"Volatility\"] = getDailyVol(ts['close'],span0=10)\n",
    "\n",
    "ts['rolling_max']=ts['close'].rolling(7).max()\n",
    "ts['rolling_min']=ts['close'].rolling(7).min()\n",
    "ts['MA_4']=ts['close'].rolling(4).mean()\n",
    "ts['MA_16']=ts['close'].rolling(16).mean()\n",
    "\n",
    "ts['Med_16']=ts['close'].rolling(16).median()\n",
    "ts['sd_4']=ts['close'].rolling(4).std()\n",
    "ts['sd_16']=ts['close'].rolling(16).std()\n",
    "ts['sk_4']=ts['close'].rolling(4).skew()\n",
    "ts['sk_16']=ts['close'].rolling(16).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96740ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=clean_dataset(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating \"from package\" tech indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ta.trend import MACD\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.volatility import AverageTrueRange\n",
    "from ta.momentum import StochasticOscillator\n",
    "from ta.trend import ADXIndicator\n",
    "from ta.trend import IchimokuIndicator\n",
    "from ta.volume import VolumeWeightedAveragePrice\n",
    "\n",
    "\n",
    "vwap = VolumeWeightedAveragePrice(ts['close'], ts['high'], ts['low'], ts['vol'])\n",
    "ts['vwap'] = vwap.volume_weighted_average_price()\n",
    "\n",
    "ichimoku = IchimokuIndicator(ts['high'], ts['low'])\n",
    "ts['tenkan_sen'] = ichimoku.ichimoku_conversion_line()\n",
    "ts['kijun_sen'] = ichimoku.ichimoku_base_line()\n",
    "ts['senkou_span_a'] = ichimoku.ichimoku_a()\n",
    "ts['senkou_span_b'] = ichimoku.ichimoku_b()\n",
    "\n",
    "\n",
    "adx = ADXIndicator(ts['high'], ts['low'], ts['close'])\n",
    "ts['adx'] = adx.adx()\n",
    "\n",
    "stoch = StochasticOscillator(ts['high'], ts['low'], ts['close'])\n",
    "ts['stoch'] = stoch.stoch()\n",
    "\n",
    "# atr = AverageTrueRange(ts['high'], ts['low'], ts['close'])\n",
    "# ts['atr'] = atr.average_true_range()\n",
    "\n",
    "bb = BollingerBands(ts['close'])\n",
    "ts['bb_upperband'] = bb.bollinger_hband()\n",
    "ts['bb_lowerband'] = bb.bollinger_lband()\n",
    "\n",
    "rsi = RSIIndicator(ts['close'])\n",
    "ts['rsi'] = rsi.rsi()\n",
    "\n",
    "macd = MACD(ts['close'])\n",
    "ts['macd'] = macd.macd()\n",
    "ts['macd_signal'] = macd.macd_signal()\n",
    "ts['macd_diff'] = macd.macd_diff()\n",
    "\n",
    "ts['atr']=ta.volatility.average_true_range(ts['high'], ts['low'], ts['close'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e54c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=clean_dataset(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780aa0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out by volume doesn't affect a lot the performance\n",
    "#ts=ts[ts['dollar_value']>ts['rolling_dollar_volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4735fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a quasi-treatment via different regimes by separting on quantile bins on different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['pct_change_bins'], pct_change_edges = pd.qcut(ts['pct_change'], q=10, labels=False, retbins=True)\n",
    "ts['CVar_4_bins'], CVar_4_bin_edges = pd.qcut(ts['CVar_4'], q=10, labels=False, retbins=True)\n",
    "ts['CVar_16_bins'], CVar_16_bin_edges = pd.qcut(ts['CVar_16'], q=10, labels=False, retbins=True)\n",
    "ts['dollar_value_ch_bins'], dollar_value_ch_bin_edges = pd.qcut(ts['dollar_value_ch'], q=10, labels=False, retbins=True)\n",
    "ts['Volatility_bins'], volatility_bin_edges = pd.qcut(ts['Volatility'], q=10, labels=False, retbins=True)\n",
    "ts['Skeness_bins'], skew_bin_edges = pd.qcut(ts['sk_4'], q=10, labels=False, retbins=True)\n",
    "ts['Macd_bins'], macd_bin_edges = pd.qcut(ts['macd'], q=10, labels=False, retbins=True)\n",
    "ts['atr_bins'], atr_bin_edges = pd.qcut(ts['atr'], q=10, labels=False, retbins=True)\n",
    "\n",
    "print(\"Pcnt_change Bin Ranges:\")\n",
    "for i in range(len(pct_change_edges) - 1):\n",
    "    print(f\"Bin {i}: {pct_change_edges[i]} - {pct_change_edges[i+1]}\")\n",
    "\n",
    "print(\"\\nCVar_4 Bin Ranges:\")\n",
    "for i in range(len(CVar_4_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {CVar_4_bin_edges[i]} - {CVar_4_bin_edges[i+1]}\")\n",
    "    \n",
    "print(\"\\nCVar_16 Bin Ranges:\")\n",
    "for i in range(len(CVar_16_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {CVar_16_bin_edges[i]} - {CVar_16_bin_edges[i+1]}\")\n",
    "      \n",
    "print(\"\\nDollar_value_ch_bin_edges  Bin Ranges:\")\n",
    "for i in range(len(dollar_value_ch_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {dollar_value_ch_bin_edges [i]} - {dollar_value_ch_bin_edges [i+1]}\") \n",
    "\n",
    "print(\"\\nVolatility  Bin Ranges:\")\n",
    "for i in range(len(volatility_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {volatility_bin_edges [i]} - {volatility_bin_edges [i+1]}\") \n",
    "    \n",
    "print(\"\\nSkeweness  Bin Ranges:\")\n",
    "for i in range(len(volatility_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {skew_bin_edges [i]} - {skew_bin_edges [i+1]}\") \n",
    "\n",
    "print(\"\\nMacd  Bin Ranges:\")\n",
    "for i in range(len(volatility_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {macd_bin_edges [i]} - {macd_bin_edges [i+1]}\") \n",
    "\n",
    "print(\"\\nATR  Bin Ranges:\")\n",
    "for i in range(len(volatility_bin_edges) - 1):\n",
    "    print(f\"Bin {i}: {atr_bin_edges [i]} - {atr_bin_edges [i+1]}\") \n",
    "   \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the data\n",
    "ts['pct_change_bins'], pct_change_edges = pd.qcut(ts['pct_change'], q=10, labels=False, retbins=True)\n",
    "ts['CVar_4_bins'], CVar_4_bin_edges = pd.qcut(ts['CVar_4'], q=10, labels=False, retbins=True)\n",
    "ts['CVar_16_bins'], CVar_16_bin_edges = pd.qcut(ts['CVar_16'], q=10, labels=False, retbins=True)\n",
    "ts['dollar_value_ch_bins'], dollar_value_ch_bin_edges = pd.qcut(ts['dollar_value_ch'], q=10, labels=False, retbins=True)\n",
    "ts['Volatility_bins'], volatility_bin_edges = pd.qcut(ts['Volatility'], q=10, labels=False, retbins=True)\n",
    "ts['Skeness_bins'], skew_bin_edges = pd.qcut(ts['sk_4'], q=10, labels=False, retbins=True)\n",
    "ts['Macd_bins'], macd_bin_edges = pd.qcut(ts['macd'], q=10, labels=False, retbins=True)\n",
    "ts['atr_bins'], atr_bin_edges = pd.qcut(ts['atr'], q=10, labels=False, retbins=True)\n",
    "\n",
    "# Displaying the bin ranges in a table\n",
    "bin_ranges = {\n",
    "    'Pcnt_change': pct_change_edges,\n",
    "    'CVar_4': CVar_4_bin_edges,\n",
    "    'CVar_16': CVar_16_bin_edges,\n",
    "    'Dollar_value_ch': dollar_value_ch_bin_edges,\n",
    "    'Volatility': volatility_bin_edges,\n",
    "    'Skewness': skew_bin_edges,\n",
    "    'Macd': macd_bin_edges,\n",
    "    'ATR': atr_bin_edges\n",
    "}\n",
    "\n",
    "bin_ranges_df = pd.DataFrame(bin_ranges)\n",
    "bin_ranges_df.index.name = 'Bin'\n",
    "bin_ranges_df.reset_index(inplace=True)\n",
    "\n",
    "bin_ranges_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further I choose best bins given there separation through visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='CVar_4_bins', y='pct_change', data=ts);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='CVar_16_bins', y='pct_change', data=ts);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='CVar_16_bins', y='pct_change', data=ts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='Volatility_bins', y='dollar_value_ch', data=ts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='Skeness_bins', y='pct_change', data=ts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00609522",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='Macd_bins', y='pct_change', data=ts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab01f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(x='atr_bins', y='pct_change', data=ts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb807dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I make default correlation on uncleaned potential features space to search for singular ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix =ts.corr()\n",
    "\n",
    "plt.figure(figsize=(30,20))  \n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "# Add title and adjust the layout\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ac9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_1=ts[[ 'vol', 'survival_probability', 'dollar_value', 'dollar_value_ch', 'log_ret',\n",
    "       'gap_OC', 'H-L_spread_change', 'O-C_spread_change', 'R_indicator',\n",
    "       'rolling_dollar_volume', 'CVar_16', 'CVar_4', 'Direction',\n",
    "       'Direction_1Lag', 'Volatility', 'sd_4', 'sd_16', 'sk_4', 'sk_16', 'vwap',\n",
    "        'adx','stoch', 'rsi', 'macd','atr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36eb013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix =ts_1.corr()\n",
    "\n",
    "plt.figure(figsize=(30,20))  \n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "# Add title and adjust the layout\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7187722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4062d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "colums_to_scale=['vol', 'survival_probability', 'dollar_value', 'dollar_value_ch',\n",
    "       'log_ret', 'gap_OC', 'H-L_spread_change', 'O-C_spread_change',\n",
    "       'R_indicator', 'rolling_dollar_volume', 'CVar_16', 'CVar_4',\n",
    "       'Direction', 'Direction_1Lag', 'Volatility', 'sd_4', 'sd_16', 'sk_4',\n",
    "       'sk_16', 'vwap', 'adx', 'stoch', 'rsi', 'macd','atr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89786f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler that is better for TS\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "ts_1 = pd.DataFrame(scaler.fit_transform(ts_1[colums_to_scale]), columns=colums_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_1['CVar_4_bins']=ts['CVar_4_bins'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aim of algpo below is to detect non-linear correlation via the powers of exponent\n",
    "# Ultimately it mainly repeats what default linear corr does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39641183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# columns_list = []\n",
    "\n",
    "# for i, col1 in enumerate(ts_1.columns):\n",
    "#     for j, col2 in enumerate(ts_1.columns):\n",
    "#         if i != j:  # Exclude diagonal elements\n",
    "#             for t in range(1, 10):\n",
    "#                 for s in range(1, 10):\n",
    "#                     t0 = t / 10\n",
    "#                     s0 = s / 10\n",
    "#                     n = 1000\n",
    "#                     A = 0\n",
    "#                     B = 0\n",
    "#                     for k in range(n):\n",
    "#                         A += math.exp(t0 * ts_1[col1][k])\n",
    "#                         B += math.exp(s0 * ts_1[col2][k])\n",
    "#                     A = A / n\n",
    "#                     B = B / n\n",
    "#                     covnl = 0\n",
    "#                     s1 = 0\n",
    "#                     s2 = 0\n",
    "#                     for k in range(n):\n",
    "#                         covnl += (math.exp(t0 * ts_1[col1][k]) - A) * (math.exp(s0 * ts_1[col2][k]) - B)\n",
    "#                         s1 += (math.exp(t0 * ts_1[col1][k]) - A) ** 2\n",
    "#                         s2 += (math.exp(s0 * ts_1[col2][k]) - B) ** 2\n",
    "#                     covnl = covnl / n\n",
    "#                     s1 = math.sqrt(s1 / n)\n",
    "#                     s2 = math.sqrt(s2 / n)\n",
    "#                     if covnl / (s1 * s2) > 0.4 or covnl / (s1 * s2) < -0.4:\n",
    "#                         print(t0, s0, col1, col2, covnl / (s1 * s2))\n",
    "#                         columns_list.append((col1, col2))\n",
    "\n",
    "# print(\"Columns that meet the criteria:\")\n",
    "# for columns in columns_list:\n",
    "#     print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_list = list(set(columns_list))\n",
    "# unique_names = set()\n",
    "# for pair in unique_list:\n",
    "#     unique_names.add(pair[0])\n",
    "#     unique_names.add(pair[1])\n",
    "\n",
    "# unique_names_list = list(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c03a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_1= clean_dataset(ts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_1['CVar_4_bins'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_2=ts_1[ts_1['CVar_4_bins']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_space = list(['vol', 'survival_probability', 'dollar_value',\n",
    "       'dollar_value_ch', 'log_ret', 'gap_OC', 'H-L_spread_change',\n",
    "       'O-C_spread_change', 'R_indicator', 'rolling_dollar_volume', 'CVar_16',\n",
    "       'CVar_4', 'Direction', 'Direction_1Lag', 'Volatility', 'sd_4', 'sd_16',\n",
    "       'sk_4', 'sk_16', 'vwap', 'adx', 'stoch', 'rsi', 'macd',\n",
    "        'atr'])\n",
    "features_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acper(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates Almost Correct Predictions Error Rate (ACPER)\n",
    "    :param y_true: list of real numbers, true values\n",
    "    :param y_pred: list of real numbers, predicted values\n",
    "    :returns: acper score\n",
    "    \"\"\"\n",
    "    threshold = 0.05\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        lower_bound = yt - (threshold * yt)\n",
    "        upper_bound = yt + (threshold * yt)\n",
    "        if (yp <= upper_bound) & (yp >= lower_bound): \n",
    "            yield True\n",
    "        else:\n",
    "            yield False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1443e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dev(dfs, feats_space, target):\n",
    "    \n",
    "    n_splits=3\n",
    "    kf = KFold(n_splits)\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(dfs)):\n",
    "        print (f\"\\n fold {fold}\")\n",
    "        print(f\" Target is: {target} \")\n",
    "        print()\n",
    "        \n",
    "\n",
    "        X_train, X_test = dfs[feats_space].iloc[train_index[:-1]], dfs[feats_space].iloc[test_index[:-1]]\n",
    "        y_train, y_test = dfs[target].iloc[train_index[1:]], dfs[target].iloc[test_index[1:]]\n",
    "        train_dataset = lgb.Dataset(X_train, y_train)\n",
    "        val_dataset = lgb.Dataset(X_test, y_test)\n",
    "\n",
    "        params = {'objective': 'mae', 'boosting_type': 'gbdt', 'max_depth': -1,'feature_fraction': .8, \n",
    "        'feature_fraction_bynode': .8, 'bagging_fraction': 0.8, 'bagging_freq': 42, 'n_jobs': -1, 'verbose': -1}\n",
    "\n",
    "        model = lgb.train(params = params, train_set = train_dataset, \n",
    "                          valid_sets = val_dataset, \n",
    "                          num_boost_round = 1000, \n",
    "                          early_stopping_rounds = 100, \n",
    "                          verbose_eval = 100)\n",
    "\n",
    "        preds=model.predict(X_test)\n",
    "        score1 = mean_absolute_error(y_test, preds)\n",
    "        shap_values = shap.TreeExplainer(model).shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "        print(np.abs(shap_values).mean(0)[0])\n",
    "        # Visualize the LightGBM tree model\n",
    "        lgb.plot_tree(model, figsize=(20, 10), show_info=['internal_count'],precision=3, orientation='horizontal')\n",
    "        plt.show()\n",
    "        \n",
    "        vals= np.abs(shap_values).mean(0)\n",
    "        f_imp = pd.DataFrame(list(zip(X_test.columns,vals)),columns=['col_name',f\"feature_importance_vals{fold}\"])\n",
    "        \n",
    "        f_imp.sort_values(by=[f\"feature_importance_vals{fold}\"],ascending=False,inplace=True)\n",
    "        f_imp.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        if fold == 0:\n",
    "            features_importance = f_imp.copy()\n",
    "        else:\n",
    "            features_importance = pd.merge(features_importance, f_imp, how = 'left', on = 'col_name')\n",
    "        \n",
    "        print(f_imp.head())\n",
    "        #scores1 += score1\n",
    "        #means =X_train.mean()\n",
    "        print(score1)\n",
    "        \n",
    "        y_true = y_test\n",
    "        y_pred = preds\n",
    "\n",
    "        T_F=list(acper(y_true, y_pred))\n",
    "        true = T_F.count(True)\n",
    "        ratio=true/len(T_F)\n",
    "        print('The ratio of almost correct to incorrect is ', ratio)\n",
    "        \n",
    "    imp_cols = [c for c in features_importance.columns if c != \"col_name\"]\n",
    "    for col in imp_cols:\n",
    "        features_importance[col] = np.abs(features_importance[col])\n",
    "    \n",
    "    #ratio=true/len(T_F)\n",
    "    features_importance['mean'] = features_importance[imp_cols].mean(axis = 1)\n",
    "    features_importance.sort_values('mean', ascending = False, inplace = True)\n",
    "    # to select least imp feature make ascending = True\n",
    "    features_importance.reset_index(drop = True, inplace = True)\n",
    "    return [features_importance['col_name'][0],features_importance['mean'][0]] # returns most important features by means across folds by shap values with a mean value\n",
    "    #return features_importance['col_name'][0],features_importance['col_name'][1] # returns 2 most imp feats\n",
    "    #return features_importance #returns all features by iteration with values\n",
    "    #return features_importance['mean'][0]\n",
    "    tree_df = model.booster_.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc164fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the whole feat space with removing target from feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l=[]\n",
    "targ=[]\n",
    "\n",
    "for f in features_space:\n",
    "    feats_space = [feat for feat in features_space if feat != f]\n",
    "    target = f\n",
    "    targ.append(f)\n",
    "    \n",
    "    l += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d066228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing 1st most important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d449ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_1=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1]\n",
    "    target = f_0\n",
    "    l_1 += [model_dev(ts_2, feats_space, target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first and second most important features are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f38057",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_2=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 and feat not in f_2]\n",
    "    target = f_0\n",
    "    l_2 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st, 2nd an 3d most important features are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_3=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    f_3=l_2[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 and feat not in f_2 and feat not in f_3]\n",
    "    target = f_0\n",
    "    l_3 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4 most important features are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_4=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    f_3=l_2[i]\n",
    "    f_4=l_3[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 \\\n",
    "                   and feat not in f_2 and feat not in f_3 and feat not in f_4]\n",
    "    target = f_0\n",
    "    l_4 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_targ=pd.DataFrame(data=[targ,l,l_1,l_2,l_3,l_4]).T\n",
    "feat_targ.columns =['Target', '1st imp feat', '2nd imp feat','3rd imp feat','4th imp feat',\\\n",
    "                   '5th imp feat']\n",
    "feat_targ['Target'] = features_space\n",
    "feat_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a4a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next the same type of procedure is done for regime on \"another end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6910c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_3=ts_1[ts_1['CVar_4_bins']==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_space = list(['vol', 'survival_probability', 'dollar_value',\n",
    "       'dollar_value_ch', 'log_ret', 'gap_OC', 'H-L_spread_change',\n",
    "       'O-C_spread_change', 'R_indicator', 'rolling_dollar_volume', 'CVar_16',\n",
    "       'CVar_4', 'Direction', 'Direction_1Lag', 'Volatility', 'sd_4', 'sd_16',\n",
    "       'sk_4', 'sk_16', 'vwap', 'adx', 'stoch', 'rsi', 'macd',\n",
    "        'atr'])\n",
    "features_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5111a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l=[]\n",
    "targ=[]\n",
    "\n",
    "for f in features_space:\n",
    "    feats_space = [feat for feat in features_space if feat != f]\n",
    "    target = f\n",
    "    targ.append(f)\n",
    "    \n",
    "    l += [model_dev(ts_3, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ef2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_1=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1]\n",
    "    target = f_0\n",
    "    l_1 += [model_dev(ts_3, feats_space, target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_2=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 and feat not in f_2]\n",
    "    target = f_0\n",
    "    l_2 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_3=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    f_3=l_2[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 and feat not in f_2 and feat not in f_3]\n",
    "    target = f_0\n",
    "    l_3 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e17202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_4=[]\n",
    "targ=[]\n",
    "\n",
    "for i in range(len(features_space)):\n",
    "    f_0 = features_space[i]\n",
    "    f_1=l[i]\n",
    "    f_2=l_1[i]\n",
    "    f_3=l_2[i]\n",
    "    f_4=l_3[i]\n",
    "    \n",
    "    feats_space = [feat for feat in features_space if feat not in f_0 and feat not in f_1 \\\n",
    "                   and feat not in f_2 and feat not in f_3 and feat not in f_4]\n",
    "    target = f_0\n",
    "    l_4 += [model_dev(ts_2, feats_space, target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace09c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_targ=pd.DataFrame(data=[targ,l,l_1,l_2,l_3,l_4]).T\n",
    "feat_targ.columns =['Target', '1st imp feat', '2nd imp feat','3rd imp feat','4th imp feat',\\\n",
    "                   '5th imp feat']\n",
    "feat_targ['Target'] = features_space\n",
    "feat_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd5143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
